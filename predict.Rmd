First, I would like to apologize for not putting this into a compiled html file, but the machine I am working on is a 2005 laptop and I can't get it to knit documents - something called pandoc is missing and it is hard to find this for my Linux distribution (which is designed for older 32bit computers.)
So I am writing this in a simple text file, you will just punish me by taking off a point when assesing this, and now let's go to the main topic.

The training data set contained over 19000 observations and about 160 variables. I did a summary of the variables and found out that about 100 of them have many missing values (NA's) for 19216 out of 19622 so I decided to exclude the variables from building the model. I also dropped the variables describing datetimes of the activities - although they would be very good predictors, I decided that I wanted a model that looks at the characteristics of the movements and not at when the subjects did their exercises.

Also, when preparing data I took into correlation between variables and detected the most correlated pairs (>0.8):
```
   correl                 var1              var2
1   0.989            roll_belt  total_accel_belt
2   0.963            roll_belt      accel_belt_y
3   0.959     total_accel_belt      accel_belt_y
4   0.939         gyros_belt_z magnet_dumbbell_y
5   0.937         yaw_dumbbell  accel_dumbbell_z
6   0.926     accel_dumbbell_x  accel_dumbbell_z
7   0.924 total_accel_dumbbell  accel_dumbbell_y
8   0.898     magnet_forearm_y  magnet_forearm_z
9   0.897        magnet_belt_y     magnet_belt_z
10  0.883       pitch_dumbbell  accel_dumbbell_x
11  0.873         gyros_belt_z      accel_belt_z
12  0.869      gyros_forearm_y   gyros_forearm_z
13  0.858         magnet_arm_y      magnet_arm_z
14  0.856      accel_forearm_y  magnet_forearm_y
15  0.850         accel_belt_x     magnet_belt_x
16  0.841            roll_belt          yaw_belt
17  0.841         gyros_belt_x magnet_dumbbell_y
18  0.804       pitch_dumbbell  accel_dumbbell_z
```
Finally I decided not to reduce the number of variables any further because I was preparing a classification model - if that should be a regression model then definitely I would give more attention to picking variables.

Here, I could describe various troubles I was having trying to make the old machine working with the random forests method. Finally though I decided to do the following:
- set control to trainControl(method = 'cv', number = 5)
- train the model using this control above with the 'gbm' algorithm (to save on memory, I used R in command line, not RStudio by the way :)

I run the model twice:

1. I divided the training set into 2 sets 75/25 (to perform the accuracy analysis):
After creating a model I compared the predicted data with actuals:
> confusionMatrix(predykcja, train1[-indeksy, 'classe'] )

```
Confusion Matrix and Statistics
          Reference
Prediction    A    B    C    D    E
         A 1350   26    0    1    0
         B    7  881   31    4   14
         C    9   21  792   26    6
         D    0    0   13  749    6
         E    1    1    2    6  856

Overall Statistics
                                          
               Accuracy : 0.9638          
                 95% CI : (0.9581, 0.9689)
    No Information Rate : 0.2847          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9541          
 Mcnemar's Test P-Value : 1.794e-06       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9876   0.9483   0.9451   0.9529   0.9705
Specificity            0.9921   0.9855   0.9844   0.9953   0.9974
Pos Pred Value         0.9804   0.9402   0.9274   0.9753   0.9885
Neg Pred Value         0.9950   0.9876   0.9883   0.9908   0.9934
Prevalence             0.2847   0.1935   0.1745   0.1637   0.1837
Detection Rate         0.2811   0.1835   0.1649   0.1560   0.1783
Detection Prevalence   0.2868   0.1951   0.1778   0.1599   0.1803
Balanced Accuracy      0.9899   0.9669   0.9647   0.9741   0.9840
```

2. Then I run the gbm model on the whole train set, this time to prepare a model for predicting output in the test dataset that was provided with the assignment. (when computing this, the processor temperature went from 55 to 77 Celcius :-)

I also checked the variable importance to chech which variables are best at explaining the movements:
> varImp(fit)

```
gbm variable importance

  only 20 most important variables shown (out of 57)

                  Overall
roll_belt         100.000
pitch_forearm      49.251
yaw_belt           39.992
magnet_dumbbell_z  29.323
magnet_dumbbell_y  23.221
roll_forearm       22.247
magnet_belt_z      18.870
accel_forearm_x    14.775
pitch_belt         14.675
gyros_belt_z       14.386
accel_dumbbell_y   12.939
roll_dumbbell      12.150
```
The final model was used to predict 20 activities that was a part of the assignment, and I scored 20/20 which confirmed that the model based on gbm was good.
Having completed this, I decided to play around with various algorithms on smaller data samples.

Here are some results, showing how many test cases would predict a model based on 1000 obs samples:
- gbm 16/20
- rf 17/20
- nb 11/20
- svmLinear 
